{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Ingénierie des données </H1>\n",
    "\n",
    "Dans ce notebook nous présentons l'une des étapes essentielle de la classification : l'ingénierie des données. Nous abordons le traitement des données catégorielles, la mise à l'échelle et le traitement des valeurs manquantes. Il existe de très nombreuses méthodes qui ont forcément un impact sur le résultat de la classification. Il est important de bien les comprendre et de rechercher celle qui est la plus adaptée en fonction du contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données catégorielles ou qualitatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De très nombreux jeux de données contiennent des données catégorielles comme une couleur, une adresse, etc. Même les classes peuvent être catégorielles (avis positif, avis négatif). De nombreux algorithmes ne sont pas capables de les traiter car ils considérent uniquement des valeurs numériques. Dans cette section nous présentons différentes manières de transformer les données catégorielles. Elles dépendent bien entendu du contexte. \n",
    "\n",
    "**Rappel** : les attributs pour lesquels il n'existe pas d'ordre sont appelés nominaux (par exemple les couleurs). En opposition ceux pour lesquels il existe un ordre sont appelés ordinaux (taille XL, L, M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons l'exemple suivant qui contient des attributs numériques et catégoriels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Taille': ['XL','L','M','S'], \n",
    "     'Couleur': ['bleu','blanc','rouge','vert'],\n",
    "     'Prix': [20.76,23.5,40.99,10.0],\n",
    "     'Classe': ['classe1','classe1','classe2','classe3']},\n",
    "    columns=['Taille','Couleur','Prix','Classe'])\n",
    "\n",
    "print ('Pour connaître les informations qui sont catégorielles, faire un df.info()')\n",
    "print (df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un jeu de données pour connaître les attributs qui ne sont pas numériques, il suffit de faire, pour un dataframe de nom df, un df.info() et les attributs non numériques apparaissent avec le type object.  Attention toutefois, lorsqu'il y a la présence de valeurs manquantes il se peut que l'attribut apparaisse avec le dtype objet. Par exemple en remplaçant le dataframe précédent par :  \n",
    "'Prix': [20.76,'nan',40.99,10.0]  \n",
    "l'information sur l'attribut prix sera : Prix       4 non-null object \n",
    "\n",
    "\n",
    "Il est également possible de faire un df.describe() qui affiche des statistiques (moyenne, max, min, etc) uniquement pour les attributs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Pour connaître quelques statistiques, faire un df.describe()')\n",
    "\n",
    "\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un df.describe() sur un attribut non numérique donne d'autres informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Couleur'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Remplacement de la valeur </H2>  \n",
    "La première approche la plus simple est de remplacer les valeurs. Considérons qu'il y ait un ordre pour les tailles tels que S=1, M=S+1 etc. Il est possible de transformer les valeurs à l'aide de la fonction map appliquée au dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la transformation\n",
    "replace_map = {'XL': 4, 'L': 3, 'M': 2, 'S': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation d'une copie de df pour ne pas perdre le df initial\n",
    "df_test=df.copy()\n",
    "\n",
    "print ('Application de la fonction map \\n')\n",
    "print (df_test['Taille'].map(replace_map))\n",
    "\n",
    "print (\"\\nAjout d'une colonne au dataframe avec application de la fonction map \\n\")\n",
    "df_test[\"Taille renommée\"]=df_test['Taille'].map(replace_map)\n",
    "\n",
    "display(df_test)       \n",
    "       \n",
    "print (\"\\nRemplacement direct de taille avec les nouvelles valeurs\\n\")\n",
    "   \n",
    "df_test.replace(replace_map, inplace=True)\n",
    "\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Label encoding </H2>\n",
    "\n",
    "Une autre approche appelée label encoding consiste à transformer l'ensemble des valeurs par un nombre de 0 au nombre-1 de catégories. Il suffit pour cela d'utiliser la fonction *LabelEncoder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#creation d'une copie de df pour ne pas perdre le df initial\n",
    "df_test=df.copy()\n",
    "\n",
    "class_label_encoder = LabelEncoder()\n",
    "print (\"\\n Affichage des transformations\\n\")\n",
    "print(class_label_encoder.fit_transform(df_test['Classe'].values))\n",
    "\n",
    "print (\"\\nAjout d'une colonne au dataframe\")\n",
    "df_test[\"Classe renommée\"]=class_label_encoder.fit_transform(df_test[\"Classe\"])\n",
    "\n",
    "display (df_test)\n",
    "\n",
    "print (\"\\nRemplacement direct de classe avec les nouvelles valeurs\\n\")\n",
    "df_test[\"Classe\"] = class_label_encoder.fit_transform(df_test[\"Classe\"])\n",
    "\n",
    "display(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> One Hot Encoding</H2>  \n",
    "\n",
    "Le label encoding a l'avantage d'être direct à obtenir mais le désavantage de donner des valeurs croissantes qui peuvent être mal considérés par les classifieurs. Par exemple dans le cas des tailles il est plus logique que XL qui est le plus grand ait une grande valeur. Par contre pour les couleurs cela ne correspond pas à une réalité. On ne peut pas dire que bleu a 4 fois plus de poids que vert par exemple.  \n",
    "\n",
    "Le principe est de convertir chaque valeur de catégorie dans une nouvelle colonne et de mettre une valeur 1 ou 0 (vrai/faux) à la colonne. Cette approche a l'avantage de ne plus mettre de poids différents pour un attribut mais à l'inconvénient de rajouter un grand nombre de colonnes si le nombre de catégories est important. \n",
    "\n",
    "Scikit learn propose la fonction *get_dummies* pour effectuer la transformation. Cette dernière prend en paramètre le dataframe, les colonnes sur lesquelles doivent s'effectuer les transformation et un prefixe qui sera utilisé pour nommer les colonnes.  \n",
    "*pd.get_dummies(df,columns=['colA','colB',..],prefix = ['leprefix'])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creation d'une copie de df pour ne pas perdre le df initial\n",
    "df_test=df.copy()\n",
    "\n",
    "print (\"\\n Affichage des transformations\\n\")\n",
    "print (pd.get_dummies(df_test[['Couleur']]))\n",
    "\n",
    "print (\"\\nAjout des colonne au dataframe\")\n",
    "df_test = pd.get_dummies(df_test,columns=['Couleur'],\n",
    "                         prefix = ['Coul'])\n",
    "\n",
    "display (df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation de données continues en données discrètes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfois il est nécessaire de devoir transformer des données continues en données discrètes. \n",
    "Par exemple si l'on a des revenues très variés il est préférable de les regrouper dans des catégories. \n",
    "Cette étape de groupement des données par classe s'appelle le binning ou la discretisation.  \n",
    "\n",
    "Scikitlearn propose la fonction KBinsDiscretizer qui permet de spécifier le nombre de groupe, le type d'encodage et la transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "print (\"Creation d'un jeu de données aléatoire de 100 lignes\")\n",
    "df_test = pd.DataFrame(\n",
    "    {'Sal': np.random.uniform(1000,10000,size=100)},columns=['Sal'])\n",
    "\n",
    "\n",
    "df_test.plot(kind='hist', figsize=(6,6),title='Histogramme')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=np.array(df_test['Sal']).reshape(-1,1)\n",
    "#KBinsDiscritizer fonctionne sur un array. \n",
    "#Comme il n'y a qu'une colonne il faut \n",
    "#faire un reshape pour lui préciser\n",
    "\n",
    "disc = KBinsDiscretizer(n_bins=5, encode='ordinal', \n",
    "                        strategy='uniform')\n",
    "df_test['Sal']=disc.fit_transform(X)\n",
    "\n",
    "df_test.plot(kind='hist', figsize=(6,6),title='Histogramme')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible de spécifier des bins spécifiques à l'aide de la fonction cut sur un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (\"Creation d'un jeu de données aléatoire de 100 lignes\")\n",
    "df_test = pd.DataFrame(\n",
    "    {'Age': np.random.randint(1, 85, size=100)},columns=['Age'])\n",
    "     \n",
    "df_test.plot(kind='hist', figsize=(6,6), title='Histogramme')\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (0, 25, 65,85)\n",
    "#Attention le nombre de label doit être inférieur au nombre de bins\n",
    "group_names = ['Jeune', 'Adulte', 'Senior']\n",
    "df_test['Age']= pd.cut(df_test['Age'], bins, labels=group_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention pd.cut transforme les données à l'aide des labels comme le montre l'exemple ci-dessous : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est donc indispensable de transformer ces données symbolique en numérique, par exemple, à l'aide de LabelEcoder ou tout autre méthode présentée précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class_label_encoder = LabelEncoder()\n",
    "print (\"\\nRemplacement direct d'age avec les nouvelles valeurs\\n\")\n",
    "df_test[\"Age\"]=class_label_encoder.fit_transform(df_test[\"Age\"])\n",
    "\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du nouvel histogramme des ages après l'étape de transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.plot(kind='hist', figsize=(6,6), title='Histogramme')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mise à l'échelle des valeurs attributs (Feature scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mise à l'échelle des valeurs des attributs (Feature scaling)  est une méthode qui est utilisée pour normaliser les tailles des valeurs des attributs. Elle est aussi appelée **normalisation** (ou **standardisation**) et constitue une étape très importante dans le pré-traitement des données notamment lorsque des distances sont utilisées. C'est le cas par exemple pour KNN, SVM, Regression, ... mais également pour des méthodes de réduction de dimensions comme PCA et même en apprentissage non supervisée (K-Means).  \n",
    "\n",
    "La normalisation est, bien entendu, effectuée attribut par attribut dans le cas où plusieurs attributs doivent être mis à l'échelle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Normalisation (ou min-max scaling)</H2>  \n",
    "\n",
    "La normalisation permet de mettre toutes les valeurs dans un intervalle de [0,1]. Elle suit la formule :  \n",
    "    $$z= \\frac{x-min(x)}{max(x)-min(x)}$$  \n",
    "    \n",
    "En scikit learn la normalisation se fait par la fonction *MinMaxScaler()*. Par défaut MinMaxScaler normalise entre 0 et 1. Il est possible de changer la valeur : *MinMaxScaler(feature_range=(0, 2))* normalisera les valeurs entre 0 et 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import numpy.random.uniform\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print (\"Création d'un dataframe de 7 valeurs\")\n",
    "data = {'Valeur': [14,-16,34,17,65,-32,5]}\n",
    "df = pd.DataFrame(data,dtype='float')\n",
    "#dtype = float car la normalisation considère \n",
    "#que les objets sont des float\n",
    "display(\"Max : \",df.max(),\" Min : \",df.min(),df)\n",
    "\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Création d'un dataframe normalisé\")\n",
    "normalise = preprocessing.MinMaxScaler()\n",
    "df_normalise = normalise.fit_transform(df)\n",
    "df_normalise = pd.DataFrame(df_normalise, columns=['Valeur'])\n",
    "\n",
    "display(\"Max : \",df_normalise.max(),\n",
    "        \" Min : \",df_normalise.min(),\n",
    "        df_normalise)\n",
    "\n",
    "df_normalise.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "normalise = preprocessing.MinMaxScaler(feature_range=(0, 2))\n",
    "df_normalise = normalise.fit_transform(df)\n",
    "df_normalise = pd.DataFrame(df_normalise, columns=['Valeur'])\n",
    "\n",
    "display(\"Max : \",df_normalise.max(),\n",
    "        \" Min : \",df_normalise.min(),\n",
    "        df_normalise)\n",
    "df_normalise.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exemple suivant illustre l'intérêt de normaliser plusieurs attributs. Il contient trois attributs où chaque valeur est prise au hasard en fonction d'une loi de distribution différente (2 assymétriques et 1 symétrique) : une avec une loi de distribution $X^2$ (https://fr.wikipedia.org/wiki/Loi_du_χ²), une avec une loi bêta (https://fr.wikipedia.org/wiki/Loi_bêta) et une avec une loi normale. L’asymétrie d’une distribution est positive si la queue de droite (à valeurs hautes) est plus longue ou grosse, et négative si la queue de gauche (à valeurs basses) est plus longue ou grosse (https://fr.wikipedia.org/wiki/Asymétrie_(statistiques)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    # asymétrie positive\n",
    "    'ChiSquare': np.random.chisquare(3, 1000)+50,\n",
    "    # asymétrie négative\n",
    "    'Beta': np.random.beta(20, 1, 1000)*30,\n",
    "    # pas d'asymétrie\n",
    "    'Normale': np.random.normal(110, 15, 1000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, \n",
    "                         columns=['ChiSquare', 'Beta', 'Normale'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 8))\n",
    "ax1.set_title('Avant le scaling')\n",
    "sns.distplot(df['ChiSquare'], ax=ax1,kde=True)\n",
    "sns.distplot(df['Beta'], ax=ax1,kde=True)\n",
    "sns.distplot(df['Normale'], ax=ax1,kde=True)\n",
    "\n",
    "ax2.set_title('Après le min-max scaling')\n",
    "sns.distplot(scaled_df['ChiSquare'], ax=ax2,kde=True)\n",
    "sns.distplot(scaled_df['Beta'], ax=ax2,kde=True)\n",
    "sns.distplot(scaled_df['Normale'], ax=ax2,kde=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons constater que les asymétries restent les mêmes mais que maintenant toutes les valeurs sont comprises entre 0 et 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Standardisation </H2>\n",
    "\n",
    "\n",
    "La standardisation est utile lorsque les attributs suivent des lois normales mais avec des moyennes et écarts type différents. Elle permet, par exemple, de rendre les algorithmes moins sensibles aux outliers.  \n",
    "\n",
    "En scikit learn la standardisation se fait par la fonction StandardScaler() en appliquant :  \n",
    "\n",
    "$$z=\\frac{x_i - \\mu}{\\sigma}\\\\\n",
    "$$\n",
    "où $\\mu$ représente la moyenne (*mean*) et $\\sigma$ l'écart type (*standard deviation*).  \n",
    "\n",
    "\n",
    "Rappel : (https://fr.wikipedia.org/wiki/Loi_normale)  \n",
    "Lorsqu'une variable aléatoire $X$ suit la loi normale, elle est dite *gaussienne* ou *normale* et il est habituel d'utiliser la notation avec la variance $\\sigma^2$ :\n",
    "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "StandardScaler  suppose donc que les données suivent une loi normale et les redimensionne pour que la distribution soit centrée autour de 0 avec un écart-type de 1.   Elle vise donc à transformer les valeurs pour qu'elles répondent à la même loi normale $$X \\sim \\mathcal{N} (0, \\, 1)$$.\n",
    "\n",
    "Il est toujours intéressant d'afficher la distribution des données pour voir si ces dernières peuvent être standardisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Valeur': [10,9,8,7,6,5,5,6,7,8,9,10]}, \n",
    "     dtype='float',\n",
    "     columns=['Valeur'])\n",
    "\n",
    "display(df.head(),\"Moyenne \",\n",
    "        df['Valeur'].mean(),\n",
    "        \" Ecart type \",\n",
    "        df['Valeur'].std())\n",
    "\n",
    "fig, ax1 = plt.subplots(ncols=1, figsize=(8, 6))\n",
    "title='X ~ N(''%0.2f'%df['Valeur'].mean()+\",%0.2f\"%df['Valeur'].std()+'^2)'\n",
    "ax1.set_title(title)\n",
    "\n",
    "sns.kdeplot(df['Valeur'], ax=ax1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print (\"Création d'un dataframe avec StandardScaler\")\n",
    "standardscaler = preprocessing.StandardScaler()\n",
    "df_standardscale = standardscaler.fit_transform(df)\n",
    "df_standardscale = pd.DataFrame(df_standardscale, \n",
    "                                columns=['Valeur'])\n",
    "\n",
    "display(df_standardscale,\"Moyenne \",\n",
    "        df_standardscale['Valeur'].mean(),\n",
    "        \" Ecart type \",\n",
    "        df_standardscale['Valeur'].std())\n",
    "\n",
    "fig, ax1 = plt.subplots(ncols=1, figsize=(8, 6))\n",
    "title='X ~ N(''%0.2f'%df_standardscale['Valeur'].mean()+\",%0.2f\"%df_standardscale['Valeur'].std()+'^2)'\n",
    "ax1.set_title(title)\n",
    "\n",
    "sns.kdeplot(df_standardscale['Valeur'], ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédémment, le code suivant illustre la standardisation avec \n",
    "$$X \\sim \\mathcal{N}(10, 2^2)$$\n",
    "$$X \\sim \\mathcal{N}(40, 7^2)$$\n",
    "$$X \\sim \\mathcal{N}(110, 15^2)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Normale1': np.random.normal(10, 2,1000),\n",
    "    'Normale2': np.random.normal(40, 7, 1000),\n",
    "    'Normale3': np.random.normal(110, 15, 1000)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "standardscaler = preprocessing.StandardScaler()\n",
    "df_standardscale = standardscaler.fit_transform(df)\n",
    "df_standardscale=pd.DataFrame(df_standardscale, \n",
    "                             columns=['Normale1', \n",
    "                                      'Normale2', \n",
    "                                      'Normale3'])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 8))\n",
    "ax1.set_title('Avant le standard scaling')\n",
    "sns.distplot(df['Normale1'], ax=ax1)\n",
    "sns.distplot(df['Normale2'], ax=ax1)\n",
    "sns.distplot(df['Normale3'], ax=ax1)\n",
    "\n",
    "ax2.set_title('Après le standard scaling')\n",
    "sns.distplot(df_standardscale['Normale1'], ax=ax2)\n",
    "sns.distplot(df_standardscale['Normale2'], ax=ax2)\n",
    "sns.distplot(df_standardscale['Normale3'], ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les données réelles, de très nombreuses fois et pour différentes raisons (données corrompues, données inexistantes, extraction incomplète, etc.) des valeurs peuvent être absentes ou apparaître sous la forme d'outlier. Généralement on parle de **valeurs manquantes** et elles peuvent proser de nombreux problèmes pour certains classifiers qui y sont très sensibles (e.g. SVM).  \n",
    "\n",
    "En fonction des domaines, elles peuvent apparaître sous la forme de -1, 0, -999 ou NaN (Not a Number). Pandas, numpy et scikit learn utilisent NaN pour les valeurs manquantes et toutes valeurs avec NaN sont ignorées dans les opérations d'agrégation comme sum, count, etc.  Il est donc préférable de remplacer toutes les valeurs manquantes par NaN.\n",
    "\n",
    "Pour remplacer des données sous la forme de NaN il suffit d'utiliser la fonction *replace()* du dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = [[7, 2, 3], [4, -1, 6], [10, 5, 9]]\n",
    "print('Remplacement valeur -1 en nan\\n')\n",
    "df = pd.DataFrame(data)\n",
    "print ('Avant :')\n",
    "display(df)\n",
    "df=df.replace(-1,np.nan)\n",
    "print ('Après :')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe différentes stratégies, en fonction des données et du domaine, pour traiter les valeurs manquantes :   \n",
    "1. Supprimer les lignes contenant des valeurs manquantes\n",
    "1. Remplacer les valeurs par *mean*, *median*, *mode*\n",
    "1. Mettre une catégorie unique\n",
    "1. Prédire la valeur manquante  \n",
    "\n",
    "Chaque stratégie a des avantages et des inconvénients. Au travers de l'exemple suivant nous illustrons les différentes stratégies et les fonctionnalités de sickit learn pour les traiter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Création de fichiers exemples\n",
    "fichier = open(\"exemplenullvalues.csv\", \"w\")\n",
    "fichier.write(\"Nom;Age;Dept;Sal;Prime\\n\")\n",
    "fichier.write(\"Marie;22;;48000;1\\n\")\n",
    "fichier.write(\"Isabelle;;Comptable;52000;0\\n\")\n",
    "fichier.write(\"Pierre;35;Informatique;;1\\n\")\n",
    "fichier.write(\"Paul;43;Commercial;49000;1\\n\")\n",
    "fichier.write(\"Jean;;Commercial;;0\\n\")\n",
    "fichier.write(\"Michel;35;;51000;0\\n\")\n",
    "fichier.write(\"Nancy;45;;66000;1\\n\")\n",
    "fichier.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour connaître le nombre de valeurs manquantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('exemplenullvalues.csv',sep=';')\n",
    "display (df)\n",
    "\n",
    "\n",
    "print ('Par rapport aux colonnes :\\n')\n",
    "display (df.info())\n",
    "print ('\\nPar rapport aux différentes lignes :\\n')\n",
    "display (df.isnull().sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour visuellement afficher si le jeu de données contient des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Supprimer les lignes contenant des valeurs manquantes</H2>  \n",
    "\n",
    "Pour supprimer simplement les lignes (ou les colonnes) qui contiennent des valeurs manquantes, il est possible d'utiliser la fonction dropna sur le dataframe :  \n",
    "*DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)*  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.copy() #pour tester\n",
    "print ('Suppression des lignes pour lesquelles au moins un élément est manquant \\n')\n",
    "print (df_test.dropna())\n",
    "\n",
    "print ('\\nSuppression des colonnes pour lesquelles au moins un élément est manquant \\n')\n",
    "print (df_test.dropna(axis=\"columns\"))\n",
    "\n",
    "print ('\\nSuppression des lignes qui ont au moins 4 valeurs non manquantes\\n')\n",
    "print (df_test.dropna(thresh=4))\n",
    "print (\"La ligne 4 n'apparait plus car elle a 2 valeurs manquantes sur les 5 colonnes\")\n",
    "\n",
    "print ('\\nRemplacement du dataframe initial en supprimant les lignes manquantes')\n",
    "df_test.dropna(inplace=True)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques**  \n",
    "La suppression des lignes permet de pouvoir utiliser des classifiers qui sont sensibles aux valeurs manquantes.  \n",
    "L'utilisation de ces méthodes risquent de supprimer de nombreuses informations (C.f. le dernier exemple). Il est conseillé de ne pas les utiliser si le nombre d'objets supprimés est trop grand. Le remplacement est généralement préférable à la suppression des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Remplacer les valeurs par mean, median, mode</H2>  \n",
    "\n",
    "Pour les variables numériques, il est possible de remplacer les valeurs manquantes par la moyenne, la médianne, le mode, etc. Rappel : dans une série le mode correspond à une valeur dominante, i.e. la valeur la plus représentée d'une variable quelconque dans une population donnée. Le choix dépend bien entendu du contexte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print ('Rappel de la colonne Age\\n', df['Age'])\n",
    "\n",
    "print ('\\nMoyenne', df['Age'].mean(),\n",
    "       ' Median',df['Age'].median(), \n",
    "       '\\nMode', df['Age'].mode())\n",
    "\n",
    "print ('\\n Remplacement des valeurs manquantes de Age par la moyenne\\n')\n",
    "df_test = df.copy() #pour tester\n",
    "print ('Avant : \\n')\n",
    "display(df_test['Age'])\n",
    "df_test['Age']=df_test['Age'].replace(np.NaN,df_test['Age'].mean())\n",
    "print ('\\nAprès : \\n')\n",
    "display(df_test['Age'])     \n",
    "\n",
    "print ('\\n Remplacement des valeurs manquantes de Age par la valeur la plus fréquente sans utiliser le mode\\n')\n",
    "df_test = df.copy() #pour tester\n",
    "print ('Valeur la plus fréquente \\n',\n",
    "       df_test['Age'].value_counts().idxmax(),\n",
    "       ' (',df_test['Age'].value_counts().max(),')')\n",
    "newval=df_test['Age'].value_counts().idxmax()\n",
    "print ('Avant : \\n')\n",
    "display(df_test['Age'])\n",
    "df_test['Age']=df_test['Age'].replace(np.NaN,newval)\n",
    "print ('\\nAprès : \\n')\n",
    "display(df_test['Age'])     \n",
    "\n",
    "print ('\\n Données catégorielles. Remplacement des valeurs manquantes de Dept par la valeur la plus fréquente\\n')\n",
    "df_test = df.copy() #pour tester\n",
    "print ('Valeur la plus fréquente',\n",
    "       df_test['Dept'].value_counts().idxmax(),\n",
    "      ' (',df_test['Dept'].value_counts().max(),')')\n",
    "newval=df_test['Dept'].value_counts().idxmax()\n",
    "print ('Avant : \\n')\n",
    "display(df_test['Dept'])\n",
    "df_test['Dept']=df_test['Dept'].replace(np.NaN,newval)\n",
    "print ('Après : \\n')\n",
    "display(df_test['Dept'])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn propose aussi une fonction *SimpleImputer* qui permet de remplacer directement les valeurs. Elle s'applique sur un tableau et non pas un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "array = df.values\n",
    "X = array[:,1:2] \n",
    "print ('\\n Remplacement des valeurs manquantes de Age par la moyenne\\n')\n",
    "print ('Avant : \\n')\n",
    "print (X)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X)\n",
    "X = imputer.transform(X)\n",
    "print ('\\nAprès : \\n')\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques**  \n",
    "Cette approche est efficace quand le jeu de données est petit et que les valeurs peuvent facilement être remplacées.  \n",
    "Le fait de faire des approximations ajoute des biais dans les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Affecter une catégorie unique </H2>  \n",
    "\n",
    "Dans le cas de variables catégorielles, lorsqu'il n'est pas possible de pouvoir connaître la valeur, il est possible d'affecter une valeur similaire aux NaN.  \n",
    "L'avantage est de pouvoir considérer toutes ces données comme étant de la même classe et ainsi elles seront transformées de la même manière que les autres valeurs de l'attribut lors de l'étape d'encodage de données catégorielles.  \n",
    "L'inconvénient est d'avoir une nouvelle classe qui ne correspond pas à grand chose et qui peut donc entraîner des problèmes lors de la classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print ('\\n Remplacement des valeurs manquantes de Dept par une valeur commune\\n')\n",
    "df_test = df.copy() #pour tester\n",
    "print ('Avant: \\n')\n",
    "display(df_test['Dept'])\n",
    "df_test['Dept']=df_test['Dept'].fillna(\"Inconnu\")\n",
    "print ('\\nAprès: \\n')\n",
    "display(df_test['Dept'])                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Prédire les valeurs manquantes </H2>  \n",
    "\n",
    "Le principe est d'utiliser les autres attributs pour appliquer un algorithme d'apprentissage en considérant que la valeur à prédire est la colonne qui contient des NaN.  \n",
    "\n",
    "L'exemple suivant illustre comment utiliser KNN pour prédire des valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (\"Creation d'un jeu de données aléatoire de 100 lignes\")\n",
    "df_test = pd.DataFrame(\n",
    "    {'Age': np.random.randint(30, 35, size=100), \n",
    "     'Sal': np.random.randint(3, size=100)*1000,\n",
    "     'Prime':np.random.randint(2, size=100),\n",
    "     'Dept': np.random.randint(3,size=100)},\n",
    "    columns=['Age','Sal','Prime','Dept'])\n",
    "     \n",
    "print (df_test.shape)\n",
    "display(df_test.head(5))\n",
    "array = df_test\n",
    "\n",
    "print ('Sélection de 3% du jeu de données pour mettre NaN dans les Dept')\n",
    "echantillon = df_test.sample(frac=0.03)\n",
    "display(echantillon)\n",
    "\n",
    "print(\"Remplacement par NaN pour l'échantillon\")\n",
    "index = echantillon.index.values\n",
    "for ind in index:\n",
    "    echantillon.at[ind, 'Dept']=-1\n",
    "    echantillon=echantillon.replace(-1,np.nan)\n",
    "    df_test.at[ind, 'Dept']=-1\n",
    "    df_test=df_test.replace(-1,np.nan)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print ('Récupération des lignes sans NAN')\n",
    "sans_nan = df_test[['Sal','Prime','Age','Dept']].notnull().all(axis=1)\n",
    "print (\"Creation d'un dataframe sans NaN\")\n",
    "df_sansnan = df_test[sans_nan]\n",
    "\n",
    "print (\"Apprentissage sur le dataframe sans les nan\")\n",
    "array = df_sansnan.values\n",
    "X = array[:,0:3]\n",
    "\n",
    "y= array[:,3]\n",
    "\n",
    "validation_size=0.25 #30% du jeu de données pour le test\n",
    "testsize= 1-validation_size\n",
    "seed=2\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=4)\n",
    "clf.fit(X, y)\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "print('\\n accuracy :', accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "#recuperation dans df_avecnan de toutes \n",
    "#les lignes qui ont un nan (noter la negation)\n",
    "print ('Remplacement des valeurs NaN par les valeurs prédites')\n",
    "df_avecnan = df_test.loc[~sans_nan].copy() \n",
    "df_avecnan['Dept'] = clf.predict(df_avecnan[['Sal','Prime','Age']])\n",
    "display(df_avecnan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_metadata": {
   "affiliation": "Montpellier University",
   "author": "P. Poncelet",
   "title": "Ingenierie des donnees"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
