{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Mieux comprendre le fonctionnement d'un classifieur </H1>\n",
    "\n",
    "Comme nous l'avons vu dans les premières classifications, le comportement des classifieurs peut être très différent. Nous présentons à présent les régions de décisions dans lesquelles le classifieur recherche les valeurs pour pouvoir prédire. L'objectif est donc ici de mieux comprendre le fonctionnement d'un classifieur, les raisons d'une bonne ou mauvaise classification et avoir une idée de l'impact des hyperparamètres.    \n",
    "\n",
    "Dans un problème de classification à deux classes, une région de décision ou surface de décision est une hypersurface qui partitionne l'espace vectoriel sous-jacent en deux ensembles : un pour chaque classe. Le classificateur classera tous les points d'un côté de la limite de décision comme appartenant à une classe et tous ceux de l'autre côté comme appartenant à l'autre classe.  \n",
    "\n",
    "Les illustrations sont faites à partir du jeu de données IRIS et nous retenons celui disponible dans scikit learn et qui a été introduit à la fin du notebook Ingénierie des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sickit learn met régulièrement à jour des versions et \n",
    "#indique des futurs warnings. \n",
    "#ces deux lignes permettent de ne pas les afficher.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# la ligne ici est ajouté principalement pour SVC dont des mises à jour\n",
    "# sont annoncées mais jamais mise à jour :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir afficher l'espace de décision, le plus simple est de se mettre dans un espace en 2 dimensions. Par la suite nous ne considérerons que les 2 attributs *sepal length* et *sepal width* dans notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print ('Lecture du fichier iris\\n')\n",
    "iris = datasets.load_iris()\n",
    "#sélection des deux attributs sepals\n",
    "X = iris.data[:, :2]  \n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des valeurs des attributs afin de voir comment elles se répartissent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Passage par un dataframe par soucis de simplification\n",
    "iris_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "iris_df['species'] = iris['target']\n",
    "\n",
    "colours = ['red', 'orange', 'blue']\n",
    "species = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "for i in range(0, 3):    \n",
    "    species_df = iris_df[iris_df['species'] == i]    \n",
    "    plt.scatter(        \n",
    "        species_df['sepal length (cm)'],        \n",
    "        species_df['sepal width (cm)'],\n",
    "        cmap=plt.cm.coolwarm, \n",
    "        color=colours[i],        \n",
    "        alpha=0.5,        \n",
    "        label=species[i]   \n",
    "    )\n",
    "\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Iris dataset: petal length vs petal width')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater Setosa est très séparée des autres classes et doit pouvoir facilement être séparé. Il est évident que la séparation entre Versicolor et Virginica va être plus difficile pour un classifieur.  \n",
    "\n",
    "\n",
    "Création d'un jeu de données d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size=0.3 #30% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de 4 classifieurs différents pour voir comment ils se comportent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(random_state=1,\n",
    "                          solver='newton-cg',\n",
    "                          multi_class='multinomial')\n",
    "\n",
    "gnb = GaussianNB()\n",
    "deci= DecisionTreeClassifier(random_state=1)\n",
    "svm = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour afficher les régions de décisions, nous utilisons la librairie mlxtend (cf notebook règles d'association) qui offre de nombreuses facilités pour afficher la zone.  \n",
    "\n",
    "Le principe consiste à parcourir la liste des classifieurs, de faire le fit, de faire la prédiction et d'afficher la zone de décision. Cette zone correspond aux différentes valeurs dans lesquelles pour une classe, le clasifieur va chercher ses valeurs. La zone est définie par rapport à l'ensemble des données. La fonction plot_decision_regions va récupérer les valeurs min et max de tous les attributs et ensuite en fonction de la classe va plutôt étendre ou modifier la zone.   \n",
    "\n",
    "Ci-dessous une fonction qui appelle plot_decision_regions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_decision_regions_iris (labels,list_clf,X_train,y_train,X_test,y_test):\n",
    "    # pour afficher les points du jeu de test plus clair\n",
    "    scatter_kwargs = {'s': 120, 'edgecolor': None, 'alpha': 0.7}\n",
    "    contourf_kwargs = {'alpha': 0.2}\n",
    "    scatter_highlight_kwargs = {'s': 120, 'label': 'Jeu de test', 'alpha': 0.7}\n",
    "    \n",
    "    #pour afficher les 4 valeurs sur 2 colonnes et 2 lignes\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    \n",
    "    for clf, label, grd in zip(list_clf,\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "        #fit du modele\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        #prediction sur le jeu de test\n",
    "        result=clf.predict(X_test)\n",
    "        acc=accuracy_score(result, y_test)\n",
    "    \n",
    "        #affichage de la zone de décision\n",
    "        ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "        fig=plot_decision_regions(X, y, clf=clf, legend=2,\n",
    "                      X_highlight=X_test,\n",
    "                      scatter_kwargs=scatter_kwargs,\n",
    "                      contourf_kwargs=contourf_kwargs,\n",
    "                      scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "        L = plt.legend()\n",
    "        L.get_texts()[0].set_text('Setosa')\n",
    "        L.get_texts()[1].set_text('Versicolor')\n",
    "        L.get_texts()[2].set_text('Virginica')\n",
    "        accu='%0.3f'%acc\n",
    "        plt.xlabel('sepal length [cm]')\n",
    "        plt.ylabel('petal length [cm]')\n",
    "        label=label+ \" (\"+accu+')'\n",
    "        plt.title(label, size=11)\n",
    "    plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez constater qu'en fonction de la stratégie de l'algorithme les zones sont très différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Logistic Regression',\n",
    "          'Naive Bayes',\n",
    "          'Decision Tree',\n",
    "          'SVM']\n",
    "list_clf=[lr,gnb,deci,svm]\n",
    "\n",
    "call_decision_regions_iris (labels,list_clf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons, à présent, tester le même classifieur mais des hyperparamètres différents. L'objectif ici est de montrer l'importance de ces choix dans un classifieur.   \n",
    "\n",
    "Dans l'exemple nous prenons le classifieur SVM. Tout d'abord avec un kernel (noyau) linéaire. Dans ce cas, la séparation entre les différentes classes se fait à l'aide de droites. Le second est LinearSVC. Il est un peu similaire au précédent mais l'implémentation est différente notamment sur la prise en compte du choix des pénalités et des fonctions de pertes. Il se comporte mieux que le précédent dans le cas de plus gros volumes de données. Le troisième est SVM avec un noyeau de type 'rbf'. Il s'agit d'une Radial Basis Function qui prend comme paramètre gamma (le paramètre qui permet de spécifier la région de décision) et C (la pénalité pour les données mal classées. Si C est petit le classifieur est ok pour les points mal classés). Enfin le dernier considére un polynome de degré trois. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0  # valeur de pénalité\n",
    "svc = SVC(kernel='linear', C=C)\n",
    "# LinearSVC (linear kernel)\n",
    "lin_svc = LinearSVC(max_iter=2000,C=C)\n",
    "# SVC avec noyau RBF\n",
    "rbf_svc = SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "# SVC avec noyeau polynomial de degre 3\n",
    "poly_svc = SVC(kernel='poly', degree=3, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = ['SVC avec un kernel linéaire',\n",
    "          'LinearSVC',\n",
    "          'SVC avec un kernel RBF',\n",
    "          'SVC avec un kernel polynomial de degré 3']\n",
    "list_clf=[svc,lin_svc,rbf_svc,poly_svc]\n",
    "call_decision_regions_iris (labels,list_clf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple suivant nous étudions différentes valeurs de gamma pour voir l'impact de SVM avec un kernel rbf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0  \n",
    "\n",
    "# SVC avec des valeurs différentes de gamma\n",
    "rbf_svc1 = SVC(kernel='rbf', gamma=1, C=C)\n",
    "rbf_svc2 = SVC(kernel='rbf', gamma=10, C=C)\n",
    "rbf_svc3 = SVC(kernel='rbf', gamma=50, C=C)\n",
    "rbf_svc4 = SVC(kernel='rbf', gamma=100, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['SVC RBF gamma=1',\n",
    "          'SVC RBF gamma=10',\n",
    "          'SVC RBF gamma=50',\n",
    "          'SVC RBF gamma=100']\n",
    "list_clf=[rbf_svc1,rbf_svc2,rbf_svc3,rbf_svc4]\n",
    "call_decision_regions_iris (labels,list_clf,X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
