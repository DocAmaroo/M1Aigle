{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Ingénierie des données textuelles </H1>  \n",
    "\n",
    "De nombreuses applications utilisent des données textuelles pour faire de la prédiction : détection d'opinions, classification automatique de documents en fonction du contenu : spam - no spam, article sport vs article économie, etc...   \n",
    "\n",
    "La classification se fait de manière tout à fait classique par contre il est indispensable de traiter les documents pour pouvoir les faire interpréter par un classifieur.\n",
    "Le traitement des données textuelles est particulièrement difficile car il dépend des données disponibles et tout traitement n'est pas forcément justifié. Par exemple le fait de convertir tout le texte en minuscule peut faire perdre de l'information (Mr Play indique une personne et play un verbe), la suppression des ponctuations peut avoir des conséquences (! est très souvent utilisé pour la détection d'opinions), etc. En outre chaque langue possède aussi ses particularités et les librairies disponibles considèrent souvent l'anglais.\n",
    "\n",
    "Il existe de nombreuses librairies qui aident à effectuer les prétraitements. Nous en présentons ici quelques unes en mettant en avant la librairie NLTK (Natural Language Toolkit) : http://www.nltk.org  \n",
    "Il est conseillé de faire un :  \n",
    "\n",
    "   import nltk  \n",
    "   nltk.download('all')  \n",
    "\n",
    "pour avoir toutes les librairies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Encodage des données **  \n",
    "\n",
    "Les données textuelles sont souvent sujettes à des problèmes d'encodage (\n",
    "“Latin”, “UTF8” etc). Le plus simple est de les convertir dans un format classique (UTF8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Kluft skrams infor pa federal electoral groe'\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "chaine = u\"Klüft skräms inför på fédéral électoral große\"\n",
    "chaine=unicodedata.normalize('NFKD', chaine).encode('ascii','ignore')\n",
    "print (chaine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Suppression des tags XML/HTML**\n",
    "\n",
    "Les données textuelles peuvent être issues de pages web, contenir des entêtes, etc..   \n",
    "L'une des premières étapes consistent à les nettoyer pour ne retenir que le texte.  \n",
    "\n",
    "La librairie BeautifulSoup permet de récupérer directement le texte en supprimant les tags : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html> <head> <title>Data-Driven Science: A New Paradigm?</title> </head>\n",
    "  <body>\n",
    "<h1>Michael L. Nelson</h1> (<a href=mln@cs.odu.edu>) is [here an example]\n",
    "</body> </html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html> <head> <title>Data-Driven Science: A New Paradigm?</title> </head>\n",
      "  <body>\n",
      "<h1>Michael L. Nelson</h1> (<a href=mln@cs.odu.edu>) is [here an example]\n",
      "</body> </html>\n"
     ]
    }
   ],
   "source": [
    "print (page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Data-Driven Science: A New Paradigm? \n",
      "\n",
      "Michael L. Nelson () is [here an example]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "page=strip_html (page)\n",
    "print (page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses modifications peuvent être réalisées à ce niveau notamment en utilisant des expressions régulières. Par exemple la fonction suivante permet de \n",
    "supprimer les textes entre crochets []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Data-Driven Science: A New Paradigm? \n",
      "\n",
      "Michael L. Nelson () is \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "page=remove_between_square_brackets(page)\n",
    "print (page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Découpage en phrase **\n",
    "\n",
    "La tokenisation consiste à découper un document en phrases ou une phrase en mots (tokens). Dans un premier temps nous découpons notre document en phrase à l'aide de sent_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\" Data Science is proving to be of paramount importance to the \"IT industry\".\n",
    "\n",
    "This is due to the increased need more than 100000 for understanding the insurmountable amount of data being produced.\n",
    "\n",
    "I'm not sure that we couldn't have a great success. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase 0 :   Data Science is proving to be of paramount importance to the \"IT industry\".\n",
      "Phrase 1 :  This is due to the increased need more than 100000 for understanding the insurmountable amount of data being produced.\n",
      "Phrase 2 :  I'm not sure that we couldn't have a great success.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "phrases = sent_tokenize(document)\n",
    "print(\"Phrase 0 : \",phrases[0])\n",
    "print(\"Phrase 1 : \",phrases[1])\n",
    "print(\"Phrase 2 : \",phrases[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Découpage en mot**\n",
    "\n",
    "La phrase est découpée en tokens à l'aide de word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'Science', 'is', 'proving', 'to', 'be', 'of', 'paramount', 'importance', 'to', 'the', '``', 'IT', 'industry', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(phrases[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons constater que les ponctuations sont considérées comme des tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mise en minuscule **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'is', 'proving', 'to', 'be', 'of', 'paramount', 'importance', 'to', 'the', '``', 'it', 'industry', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.lower() for w in tokens]\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transformation des numériques en mots **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre à convertir \n",
      "\n",
      "['100000']\n",
      "Nombre après conversion \n",
      "\n",
      "['one hundred thousand']\n"
     ]
    }
   ],
   "source": [
    "#inflect est une librairie qui permet de convertir les nombres en mots\n",
    "import inflect\n",
    "\n",
    "tokens = word_tokenize(phrases[1])\n",
    "\n",
    "print (\"Nombre à convertir \\n\")\n",
    "words = [word for word in tokens if word.isdigit()]\n",
    "print(words)\n",
    "p = inflect.engine()\n",
    "numbertransf = [p.number_to_words(word) for word in tokens if word.isdigit()]\n",
    "\n",
    "print (\"Nombre après conversion \\n\")\n",
    "print(numbertransf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Suppression des ponctuations **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'due', 'to', 'the', 'increased', 'need', 'more', 'than', 'for', 'understanding', 'the', 'insurmountable', 'amount', 'of', 'data', 'being', 'produced']\n"
     ]
    }
   ],
   "source": [
    "# Suppression de tous les termes qui ne sont pas alphanumériques\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Traitement des contractions **\n",
    "\n",
    "La dernière phrase contient des contractions. Voici ce que cela donne lorsque l'on obtient des tokens et que l'on supprime les caractères non alphabétiques. Des parties de négation disparaissent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'not', 'sure', 'that', 'we', 'could', \"n't\", 'have', 'a', 'great', 'success', '.']\n",
      "['I', 'not', 'sure', 'that', 'we', 'could', 'have', 'a', 'great', 'success']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(phrases[2])\n",
    "print(tokens)\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant remplacement\n",
      "\n",
      "I'm not sure that we couldn't have a great success.\n",
      "\n",
      "Après remplacement\n",
      "\n",
      "I am not sure that we could not have a great success.\n",
      "['I', 'am', 'not', 'sure', 'that', 'we', 'could', 'not', 'have', 'a', 'great', 'success', '.']\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "print (\"Avant remplacement\\n\")\n",
    "print (phrases[2])\n",
    "print (\"\\nAprès remplacement\\n\")\n",
    "laphrase=replace_contractions(phrases[2])\n",
    "print (laphrase)\n",
    "tokens = word_tokenize(laphrase)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Suppression des stop words **\n",
    "\n",
    "Les stopwords sont des mots qui n'ont pas beaucoup d'intérêt dans la classification. Il s'agit des mots comme the, as etc. Attention il faut toujours faire attention à la liste des stopwords. Certains d'entre eux sont peut être utiles pour l'analyse. Le fait de supprimer is peut manquer par la suite. De plus il est parfois utile de mettre dans la liste des stopwords les mots très courant du domaine. Si les données parlent toutes de cinéma il faut mettre cinéma dans la liste des stopwords.   \n",
    "\n",
    "NLTK propose une liste de stopwords pour différentes langues. Ils sont en minuscule et avec des contractions. Pour les utiliser il faut donc avoir fait le même traitement sur nos données.  \n",
    "Il existe de nombreux sites qui proposent des listes de stopwords en différentes langues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "stop_words = stopwords.words('french')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'application des stopwords sur la première phrase\n",
      "\n",
      "Avant transformation \n",
      "\n",
      "['data', 'science', 'is', 'proving', 'to', 'be', 'of', 'paramount', 'importance', 'to', 'the', 'it', 'industry']\n",
      "\n",
      "Après transformation \n",
      "\n",
      "['data', 'science', 'proving', 'paramount', 'importance', 'industry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print (\"Exemple d'application des stopwords sur la première phrase\\n\")\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print (\"Avant transformation \\n\")\n",
    "print (words)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print (\"\\nAprès transformation \\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stemmatisation **\n",
    "\n",
    "La stemmatisation (stemming ou racinisation) est l'opération qui consiste à réduire chaque mot à sa racine. Par exemple malad, maladie, maladive en maladive. Les approches utilisent l'algorithme de Porter qui se base sur le suffixe des mots. NTLK propose la librairie SnowballStemmer pour le français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'scienc', 'is', 'prove', 'to', 'be', 'of', 'paramount', 'import', 'to', 'the', '``', 'it', 'industri', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dat', 'sci', 'is', 'prov', 'to', 'be', 'of', 'paramount', 'import', 'to', 'the', '``', 'it', 'industry', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "# un autre stemmatiser\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lstemmed = [lancaster_stemmer.stem(word) for word in tokens]\n",
    "print(lstemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant transformation \n",
      "\n",
      "['malade', 'malades', 'maladie', 'maladies', 'maladive']\n",
      "\n",
      " Après transformation\n",
      "\n",
      "['malad', 'malad', 'malad', 'malad', 'malad']\n"
     ]
    }
   ],
   "source": [
    "# un autre stemmatiseur qui accepte le français\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "phrase = \"malade malades maladie maladies maladive\"\n",
    "tokens = word_tokenize(phrase)\n",
    "print (\"Avant transformation \\n\")\n",
    "print (tokens)\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "print (\"\\n Après transformation\\n\")\n",
    "print (stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** La lemmatisation **  \n",
    "\n",
    "La lemmatisation consiste aussi à réduire chaque mot à sa racine mais par contre elle ne va pas s'intéresser au suffixe du mot. Elle effectue une première analyse pour mettre les verbes à l'infinitif, supprimer les s pour les pluriels.   \n",
    "\n",
    "Le choix de l'une ou de l'autre méthode dépend bien entendu de la tâche de classification que l'on souhaite faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation \n",
      "\n",
      "Lemmatisation : \n",
      " ['data', 'science', 'be', 'prove', 'to', 'be', 'of', 'paramount', 'importance', 'to', 'the', '``', 'it', 'industry', \"''\", '.']\n",
      "\n",
      " A comparer avec la Stemmatisation\n",
      "\n",
      "['dat', 'sci', 'is', 'prov', 'to', 'be', 'of', 'paramount', 'import', 'to', 'the', '``', 'it', 'industry', \"''\", '.']\n",
      "Lemmatisation : \n",
      " ['dat', 'sci', 'is', 'prov', 'to', 'be', 'of', 'paramount', 'import', 'to', 'the', '``', 'it', 'industry', \"''\", '.']\n",
      "\n",
      " autre exemple avec la phrase\n",
      "\n",
      "have had having dogs crying\n",
      "Stemmatisation : \n",
      " ['have', 'had', 'have', 'dog', 'cri']\n",
      "Lemmatisation : \n",
      " ['have', 'have', 'have', 'dog', 'cry']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "print (\"Lemmatisation \\n\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lstemmed = [wordnet_lemmatizer.lemmatize(word,pos='v') for word in tokens]\n",
    "print(\"Lemmatisation : \\n\",lstemmed)\n",
    "\n",
    "print (\"\\n A comparer avec la Stemmatisation\\n\")\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lstemmed = [lancaster_stemmer.stem(word) for word in tokens]\n",
    "print(lstemmed)\n",
    "\n",
    "\n",
    "print(\"Lemmatisation : \\n\",lstemmed)\n",
    "print (\"\\n autre exemple avec la phrase\\n\")\n",
    "sentence = \"have had having dogs crying\"\n",
    "print (sentence)\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(\"Stemmatisation : \\n\",stemmed)\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lstemmed = [wordnet_lemmatizer.lemmatize(word,pos='v') for word in tokens]\n",
    "print(\"Lemmatisation : \\n\",lstemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part of speech tagging **  \n",
    "\n",
    "Cette étape consiste à appliquer un analyseur morpho-syntaxique pour déterminer le genre du mot dans la phrase. Elle peut être très utile pour, par exemple, ne considérer que les adjectifs, les verbes ou les adverbes dans le cas de \n",
    "la détection de l'opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('proving', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('of', 'IN'),\n",
       " ('paramount', 'NN'),\n",
       " ('importance', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " ('industry', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de connaître l'intitulé de chaque Tag par la fonction nltk.help.upenn_tagset(‘XX’) où XX représente le Tag recherché. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (nltk.help.upenn_tagset('RB'))\n",
    "print (nltk.help.upenn_tagset('VB'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'intégrer le genre directement avec le mot à l'aide de la fonction pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'NNP'), ('Science', 'NNP'), ('is', 'VBZ'), ('proving', 'VBG'), ('to', 'TO'), ('be', 'VB'), ('of', 'IN'), ('paramount', 'NN'), ('importance', 'NN'), ('to', 'TO'), ('the', 'DT'), ('``', '``'), ('IT', 'NNP'), ('industry', 'NN'), (\"''\", \"''\"), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "\n",
    "tokens=pos_tag(tokens)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Données de type tweet **\n",
    "\n",
    "Les tweets ont une syntaxe très particulière et généralement les traitements se font à l'aide d'expressions régulières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = '#NLP is thus an example :D ;) RT @theUser: see http://example.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un exemple de tweet : \n",
      " #NLP is thus an example :D RT @theUser: see http://example.com\n",
      "\n",
      "Le tweet avec un processus normal de transformation\n",
      "\n",
      "['#', 'NLP', 'is', 'thus', 'an', 'example', ':', 'D', 'RT', '@', 'theUser', ':', 'see', 'http', ':', '//example.com']\n",
      "\n",
      "Le tweet avec des expressions régulières\n",
      "\n",
      "['#NLP', 'is', 'thus', 'an', 'example', ':D', 'RT', '@theUser', ':', 'see', 'http://example.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#traitement des émoticones\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "#Prise en compte des éléments qui doivent être regroupés\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # nombres\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # mots avec - et '\n",
    "    r'(?:[\\w_]+)', # autres mots\n",
    "    r'(?:\\S)' # le reste\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "# un example de tweet\n",
    "tweet = '#NLP is thus an example :D RT @theUser: see http://example.com'\n",
    "print (\"Un exemple de tweet : \\n\",tweet)\n",
    "\n",
    "print (\"\\nLe tweet avec un processus normal de transformation\\n\")\n",
    "print (word_tokenize(tweet))       \n",
    "print (\"\\nLe tweet avec des expressions régulières\\n\")       \n",
    "words=preprocess(tweet)       \n",
    "print(words)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
